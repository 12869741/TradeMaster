{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Here is the jupyter notebook for those who have signed up for the competition in NTU.\n",
    "\n",
    "The competition's goal is to maximize the cummulative environment-given reward.\n",
    "The key differenet of RL from traditional supervised learning is the loss(here indicates the cummulative reward) could not be optimized directly and therefore the updates of the nets(agents) is a little different.\n",
    "\n",
    "We either use policy-based methods where we integrate the possibility of taking the action and the reward together to update or we use value based method where a judger will determine whether a single state or action is good.\n",
    "\n",
    "In this competition, I will provide a RL environment which an agent can interact with and RL baseline. Besides, I am going to uncover the core code the RL baseline and show you the how you can build your own agent. You can either choose to build your own agent or you can adjust the hyperparameters to achieve the most cumulative reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "Here we provide the [video tutorial](https://www.youtube.com/watch?v=7rtqFT9I4uo&t=12s) for you to install this project to better participate in the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL enviornment & baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to add the project to our system path because the setting of jupter notebook is a little different from that of py document.\n",
    "Then, we need to load the document we need to import the module we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from agent.EIIE.model import EIIE_con, EIIE_lstm, EIIE_rnn, EIIE_critirc\n",
    "import argparse\n",
    "from agent.EIIE.util import *\n",
    "from env.PM.portfolio_for_EIIE import Tradingenv\n",
    "from logging import raiseExceptions\n",
    "from stat import S_ENFMT\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import sys\n",
    "from agent.EIIE.trader import trader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a part where you can adjust the default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--random_seed\",\n",
    "                    type=int,\n",
    "                    default=12345,\n",
    "                    help=\"the path for storing the downloaded data\")\n",
    "parser.add_argument(\n",
    "    \"--env_config_path\",\n",
    "    type=str,\n",
    "    default=\"config/input_config/env/portfolio/portfolio_for_EIIE/\",\n",
    "    help=\"the path for storing the downloaded data\")\n",
    "parser.add_argument(\n",
    "    \"--net_type\",\n",
    "    choices=[\"conv\", \"lstm\", \"rnn\"],\n",
    "    default=\"conv\",\n",
    "    help=\"the name of the model\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_hidden_nodes\",\n",
    "    type=int,\n",
    "    default=32,\n",
    "    help=\"the number of hidden nodes in lstm or rnn\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_out_channel\",\n",
    "    type=int,\n",
    "    default=2,\n",
    "    help=\"the number of channel\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--gamma\",\n",
    "    type=float,\n",
    "    default=0.99,\n",
    "    help=\"the gamma for DPG\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model_path\",\n",
    "    type=str,\n",
    "    default=\"result/EIIE/trained_model\",\n",
    "    help=\"the path for trained model\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--result_path\",\n",
    "    type=str,\n",
    "    default=\"result/EIIE/test_result\",\n",
    "    help=\"the path for test result\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epoch\",\n",
    "    type=int,\n",
    "    default=10,\n",
    "    help=\"the number of epoch we train\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=[])\n",
    "agent=trader(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train_with_valid()\n",
    "agent.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next is core code of the agent.\n",
    "## RL environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env_instance=agent.train_env_instance\n",
    "valid_env_instance=agent.valid_env_instance\n",
    "test_env_instance=agent.train_env_instance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting for RL environment\n",
    "\n",
    "There are 3 RL environment in all, if you want to build and train your own agent, you can train it in the train_env_instance, pick the best model for the valid_env_instance and back test in the test_env_instance.\n",
    "\n",
    "The action space of the environment is a 30-dimension numpy array, which represents the score of cash+29 stocks.\n",
    "\n",
    "The observation space of the environment is numpy array of the shape (29, 10, 11), which represents the number of the tickers, the length of the day and the number of features respectively, which means that there are 29 stocks and each state contains the daily price information for 10 days, and the price information contains 11 technical indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reseting the environment, means to clear all the history and start from the begining. \n",
    "# It will return the initial state  \n",
    "# Here is an example of posing the random action to the train_env_instance\n",
    "s=train_env_instance.reset()\n",
    "action=np.random.rand(16)\n",
    "done=False\n",
    "while not done:\n",
    "    old_state = s\n",
    "    s, reward, done, _ =train_env_instance.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next is the core code of EIIE, I will decompose it so that you can understand the process of training, making it easier for you to build your own agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the net\n",
    "# Just like the supervised leanrning process, we need a net to regress something, here the EIIE is Actor-Critic RL model where we need a actor to to generate policy and a critic\\\n",
    "# to judge whether the state is good or not\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class EIIE_con(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, length, kernel_size=3):\n",
    "        super(EIIE_con, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.length = length\n",
    "        self.act = torch.nn.ReLU(inplace=False)\n",
    "        self.con1d = nn.Conv1d(self.in_channels,\n",
    "                               self.out_channels,\n",
    "                               kernel_size=3)\n",
    "        self.con2d = nn.Conv1d(self.out_channels,\n",
    "                               1,\n",
    "                               kernel_size=self.length - self.kernel_size + 1)\n",
    "        self.con3d = nn.Conv1d(1, 1, kernel_size=1)\n",
    "        self.para = torch.nn.Parameter(torch.ones(1).requires_grad_())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.con1d(x)\n",
    "        x = self.act(x)\n",
    "        x = self.con2d(x)\n",
    "        x = self.act(x)\n",
    "        x = self.con3d(x)\n",
    "        x = x.view(-1)\n",
    "\n",
    "        # self.linear2 = nn.Linear(len(x), len(x) + 1)\n",
    "        # x = self.linear2(x)\n",
    "        x = torch.cat((x, self.para), dim=0)\n",
    "        x = torch.softmax(x, dim=0)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class EIIE_lstm(nn.Module):\n",
    "    def __init__(self, n_features, layer_num, n_hidden):\n",
    "        super(EIIE_lstm, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = layer_num\n",
    "        self.lstm = nn.LSTM(input_size=n_features,\n",
    "                            hidden_size=self.n_hidden,\n",
    "                            num_layers=self.n_layers,\n",
    "                            batch_first=True)\n",
    "        self.linear = nn.Linear(self.n_hidden, 1)\n",
    "        self.con3d = nn.Conv1d(1, 1, kernel_size=1)\n",
    "        self.para = torch.nn.Parameter(torch.ones(1).requires_grad_())\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        x = self.linear(lstm_out[:, -1, :]).view(-1, 1, 1)\n",
    "        x = self.con3d(x)\n",
    "        x = x.view(-1)\n",
    "        x = torch.cat((x, self.para), dim=0)\n",
    "        x = torch.softmax(x, dim=0)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EIIE_rnn(nn.Module):\n",
    "    def __init__(self, n_features, layer_num, n_hidden):\n",
    "        super(EIIE_rnn, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = layer_num\n",
    "        self.rnn = nn.RNN(input_size=n_features,\n",
    "                          hidden_size=self.n_hidden,\n",
    "                          num_layers=self.n_layers,\n",
    "                          batch_first=True)\n",
    "        self.linear = nn.Linear(self.n_hidden, 1)\n",
    "        self.con3d = nn.Conv1d(1, 1, kernel_size=1)\n",
    "        self.para = torch.nn.Parameter(torch.ones(1).requires_grad_())\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.rnn(x)\n",
    "        x = self.linear(lstm_out[:, -1, :]).view(-1, 1, 1)\n",
    "        x = self.con3d(x)\n",
    "        x = x.view(-1)\n",
    "        x = torch.cat((x, self.para), dim=0)\n",
    "        x = torch.softmax(x, dim=0)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EIIE_critirc(nn.Module):\n",
    "    def __init__(self, n_features, layer_num, n_hidden):\n",
    "        super(EIIE_critirc, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = layer_num\n",
    "        self.lstm = nn.LSTM(input_size=n_features,\n",
    "                            hidden_size=self.n_hidden,\n",
    "                            num_layers=self.n_layers,\n",
    "                            batch_first=True)\n",
    "        self.linear = nn.Linear(self.n_hidden, 1)\n",
    "        self.con3d = nn.Conv1d(1, 1, kernel_size=1)\n",
    "        self.para = torch.nn.Parameter(torch.ones(1).requires_grad_())\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        x = self.linear(lstm_out[:, -1, :]).view(-1, 1, 1)\n",
    "        x = self.con3d(x)\n",
    "        x = x.view(-1)\n",
    "        x = torch.cat((x, self.para, a), dim=0)\n",
    "        x = torch.nn.ReLU(inplace=False)(x)\n",
    "        number_nodes = len(x)\n",
    "        self.linear2 = nn.Linear(number_nodes, 1)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are the code for the trader, the key lies on the learn function\n",
    "class trader:\n",
    "    def __init__(self):\n",
    "        self.num_epoch = 10\n",
    "        self.GPU_IN_USE = torch.cuda.is_available()\n",
    "        self.device = torch.device('cpu' if self.GPU_IN_USE else 'cpu')\n",
    "        self.model_path = \"result/EIIE/trained_model\"\n",
    "        if not os.path.exists(self.model_path):\n",
    "            os.makedirs(self.model_path)\n",
    "        self.result_path = \"result/EIIE/test_result\"\n",
    "        if not os.path.exists(self.result_path):\n",
    "            os.makedirs(self.result_path)\n",
    "        self.train_env_instance = train_env_instance\n",
    "        self.valid_env_instance = valid_env_instance\n",
    "        self.test_env_instance = test_env_instance\n",
    "        self.day_length = 10\n",
    "        self.input_channel = 11\n",
    "        self.net = EIIE_con(self.input_channel, 2,\n",
    "                           self.day_length)\n",
    "        \n",
    "        self.critic = EIIE_critirc(self.input_channel, 1,\n",
    "                                  32)\n",
    "        self.test_action_memory = []  # to store the\n",
    "        self.optimizer_actor = torch.optim.Adam(self.net.parameters(), lr=1e-4)\n",
    "        self.optimizer_critic = torch.optim.Adam(self.critic.parameters(),\n",
    "                                                 lr=1e-4)\n",
    "        self.memory_counter = 0\n",
    "        self.memory_capacity = 1000\n",
    "        self.s_memory = []\n",
    "        self.a_memory = []\n",
    "        self.r_memory = []\n",
    "        self.sn_memory = []\n",
    "        self.policy_update_frequency = 500\n",
    "        self.critic_learn_time = 0\n",
    "        self.gamma = 0.99\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.net = self.net.to(self.device)\n",
    "        self.critic = self.critic.to(self.device)\n",
    "\n",
    "    def store_transition(\n",
    "        self,\n",
    "        s,\n",
    "        a,\n",
    "        r,\n",
    "        s_,\n",
    "    ):  # 定义记忆存储函数 (这里输入为一个transition)\n",
    "\n",
    "        self.memory_counter = self.memory_counter + 1\n",
    "        if self.memory_counter < self.memory_capacity:\n",
    "            self.s_memory.append(s)\n",
    "            self.a_memory.append(a)\n",
    "            self.r_memory.append(r)\n",
    "            self.sn_memory.append(s_)\n",
    "        else:\n",
    "            number = self.memory_counter % self.memory_capacity\n",
    "            self.s_memory[number - 1] = s\n",
    "            self.a_memory[number - 1] = a\n",
    "            self.r_memory[number - 1] = r\n",
    "            self.sn_memory[number - 1] = s_\n",
    "\n",
    "    def compute_single_action(self, state):\n",
    "        state = torch.from_numpy(state).float().to(self.device)\n",
    "        action = self.net(state)\n",
    "        action = action.detach().cpu().numpy()\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        # here is the core of the trader, it shows how the updates coming out\n",
    "        # we first need to have some stored the transcation(s,a,r,s_) \n",
    "        length = len(self.s_memory)\n",
    "        out1 = random.sample(range(length), int(length / 10))\n",
    "        # random sample\n",
    "        s_learn = []\n",
    "        a_learn = []\n",
    "        r_learn = []\n",
    "        sn_learn = []\n",
    "        for number in out1:\n",
    "            s_learn.append(self.s_memory[number])\n",
    "            a_learn.append(self.a_memory[number])\n",
    "            r_learn.append(self.r_memory[number])\n",
    "            sn_learn.append(self.sn_memory[number])\n",
    "        self.critic_learn_time = self.critic_learn_time + 1\n",
    "        # for the transcation we have stored, we need to update the actor and critic\n",
    "        # for the actor, we need to comput the action and use the critic to judge the action\n",
    "        # we need to update the actor so that for every action it choose, it can gain more scores from a critic than other action \n",
    "        # for the critic , we simply use the td_error to update it because it is MDP\n",
    "\n",
    "        for bs, ba, br, bs_ in zip(s_learn, a_learn, r_learn, sn_learn):\n",
    "            #update actor\n",
    "            a = self.net(bs)\n",
    "            q = self.critic(bs, a)\n",
    "            a_loss = -torch.mean(q)\n",
    "            self.optimizer_actor.zero_grad()\n",
    "            a_loss.backward(retain_graph=True)\n",
    "            self.optimizer_actor.step()\n",
    "            #update critic\n",
    "            a_ = self.net(bs_)\n",
    "            q_ = self.critic(bs_, a_.detach())\n",
    "            q_target = br + self.gamma * q_\n",
    "            q_eval = self.critic(bs, ba.detach())\n",
    "            # print(q_eval)\n",
    "            # print(q_target)\n",
    "            td_error = self.mse_loss(q_target.detach(), q_eval)\n",
    "            # print(td_error)\n",
    "            self.optimizer_critic.zero_grad()\n",
    "            td_error.backward()\n",
    "            self.optimizer_critic.step()\n",
    "\n",
    "    def train_with_valid(self):\n",
    "        rewards_list = []\n",
    "        for i in range(self.num_epoch):\n",
    "            j = 0\n",
    "            done = False\n",
    "            s = self.train_env_instance.reset()\n",
    "            while not done:\n",
    "\n",
    "                old_state = s\n",
    "                action = self.net(torch.from_numpy(s).float())\n",
    "                s, reward, done, _ = self.train_env_instance.step(\n",
    "                    action.detach().numpy())\n",
    "                self.store_transition(\n",
    "                    torch.from_numpy(old_state).float().to(self.device),\n",
    "                    action,\n",
    "                    torch.tensor(reward).float().to(self.device),\n",
    "                    torch.from_numpy(s).float().to(self.device))\n",
    "                j = j + 1\n",
    "                if j % 200 == 1:\n",
    "\n",
    "                    self.learn()\n",
    "            all_model_path = self.model_path + \"/all_model/\"\n",
    "            best_model_path = self.model_path + \"/best_model/\"\n",
    "            if not os.path.exists(all_model_path):\n",
    "                os.makedirs(all_model_path)\n",
    "            if not os.path.exists(best_model_path):\n",
    "                os.makedirs(best_model_path)\n",
    "            torch.save(self.net,\n",
    "                       all_model_path + \"actor_num_epoch_{}.pth\".format(i))\n",
    "            torch.save(self.critic,\n",
    "                       all_model_path + \"critic_num_epoch_{}.pth\".format(i))\n",
    "            s = self.valid_env_instance.reset()\n",
    "            done = False\n",
    "            rewards = 0\n",
    "            while not done:\n",
    "\n",
    "                old_state = s\n",
    "                action = self.net(torch.from_numpy(s).float())\n",
    "                s, reward, done, _ = self.valid_env_instance.step(\n",
    "                    action.detach().numpy())\n",
    "                rewards = rewards + reward\n",
    "            rewards_list.append(rewards)\n",
    "        index = rewards_list.index(np.max(rewards_list))\n",
    "        actor_model_path = all_model_path + \"actor_num_epoch_{}.pth\".format(\n",
    "            index)\n",
    "        critic_model_path = all_model_path + \"critic_num_epoch_{}.pth\".format(\n",
    "            index)\n",
    "        self.net = torch.load(actor_model_path)\n",
    "        self.critic = torch.load(critic_model_path)\n",
    "        torch.save(self.net, best_model_path + \"actor.pth\")\n",
    "        torch.save(self.critic, best_model_path + \"critic.pth\")\n",
    "\n",
    "    def test(self):\n",
    "        s = self.test_env_instance.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            old_state = s\n",
    "            action = self.net(torch.from_numpy(s).float())\n",
    "            s, reward, done, _ = self.test_env_instance.step(\n",
    "                action.detach().numpy())\n",
    "        df_return = self.test_env_instance.save_portfolio_return_memory()\n",
    "        df_assets = self.test_env_instance.save_asset_memory()\n",
    "        assets = df_assets[\"total assets\"].values\n",
    "        daily_return = df_return.daily_return.values\n",
    "        df = pd.DataFrame()\n",
    "        df[\"daily_return\"] = daily_return\n",
    "        df[\"total assets\"] = assets\n",
    "        if not os.path.exists(self.result_path):\n",
    "            os.makedirs(self.result_path)\n",
    "        df.to_csv(self.result_path + \"/result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=trader()\n",
    "agent.train_with_valid()\n",
    "agent.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('TradeMaster')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a1f97403911abd3f02553c8f2b0c54537fddc7efadd9f5d3e31784db6e40c347"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
